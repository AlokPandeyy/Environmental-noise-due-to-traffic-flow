{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no NaN values in the Excel sheet.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"C:/Users/Alok Kumar Pandey/Downloads/Book1.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Check for NaN values\n",
    "nan_check = df.isnull().values.any()\n",
    "\n",
    "if nan_check:\n",
    "    print(\"There are NaN values in the Excel sheet.\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the Excel sheet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your features and target variable (assuming 'target' is your target column)\n",
    "X = df.drop(columns=['Leq'])  # Replace 'target' with your actual target column name\n",
    "y = df['Leq']  # Replace 'target' with your actual target column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 368\n",
      "Test set size: 92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into train (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the train and test sets\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "                      Leq       L10       L50       L90    Speed        Car  \\\n",
      "Leq             1.000000  0.867535  0.883498  0.858651 -0.145071  0.600849   \n",
      "L10             0.867535  1.000000  0.963669  0.956383 -0.126770  0.528490   \n",
      "L50             0.883498  0.963669  1.000000  0.984782 -0.133415  0.645415   \n",
      "L90             0.858651  0.956383  0.984782  1.000000 -0.117051  0.587749   \n",
      "Speed          -0.145071 -0.126770 -0.133415 -0.117051  1.000000 -0.050590   \n",
      "Car             0.600849  0.528490  0.645415  0.587749 -0.050590  1.000000   \n",
      "2w              0.487152  0.378598  0.508919  0.433392 -0.238409  0.756933   \n",
      "3w             -0.220730 -0.353472 -0.274342 -0.335998  0.051276  0.132580   \n",
      "LCV             0.593836  0.603268  0.591789  0.565142 -0.058767  0.467244   \n",
      "Heavy traffic   0.691045  0.781455  0.730931  0.738651  0.002463  0.356019   \n",
      "%HT             0.095292  0.238431  0.103548  0.170841  0.148986 -0.417880   \n",
      "Median width    0.386782  0.257237  0.410012  0.324055 -0.285025  0.671823   \n",
      "Shoulder width  0.836783  0.839548  0.899342  0.849123 -0.221596  0.721356   \n",
      "Temp.           0.318897  0.206144  0.369221  0.289518 -0.140497  0.600506   \n",
      "RH             -0.120845 -0.063198 -0.184892 -0.146577 -0.021427 -0.312738   \n",
      "WS              0.203369  0.154649  0.234617  0.228651 -0.082475  0.269288   \n",
      "\n",
      "                      2w        3w       LCV  Heavy traffic       %HT  \\\n",
      "Leq             0.487152 -0.220730  0.593836       0.691045  0.095292   \n",
      "L10             0.378598 -0.353472  0.603268       0.781455  0.238431   \n",
      "L50             0.508919 -0.274342  0.591789       0.730931  0.103548   \n",
      "L90             0.433392 -0.335998  0.565142       0.738651  0.170841   \n",
      "Speed          -0.238409  0.051276 -0.058767       0.002463  0.148986   \n",
      "Car             0.756933  0.132580  0.467244       0.356019 -0.417880   \n",
      "2w              1.000000  0.185864  0.319913       0.160126 -0.594112   \n",
      "3w              0.185864  1.000000 -0.117041      -0.299297 -0.403069   \n",
      "LCV             0.319913 -0.117041  1.000000       0.654993  0.151933   \n",
      "Heavy traffic   0.160126 -0.299297  0.654993       1.000000  0.586636   \n",
      "%HT            -0.594112 -0.403069  0.151933       0.586636  1.000000   \n",
      "Median width    0.855141  0.241433  0.276779       0.023944 -0.645022   \n",
      "Shoulder width  0.708688 -0.145658  0.600918       0.614010 -0.129868   \n",
      "Temp.           0.740045  0.183729  0.217199       0.032755 -0.532408   \n",
      "RH             -0.353233 -0.044675 -0.074841       0.003525  0.270855   \n",
      "WS              0.307379 -0.009502  0.193743       0.177434 -0.049840   \n",
      "\n",
      "                Median width  Shoulder width     Temp.        RH        WS  \n",
      "Leq                 0.386782        0.836783  0.318897 -0.120845  0.203369  \n",
      "L10                 0.257237        0.839548  0.206144 -0.063198  0.154649  \n",
      "L50                 0.410012        0.899342  0.369221 -0.184892  0.234617  \n",
      "L90                 0.324055        0.849123  0.289518 -0.146577  0.228651  \n",
      "Speed              -0.285025       -0.221596 -0.140497 -0.021427 -0.082475  \n",
      "Car                 0.671823        0.721356  0.600506 -0.312738  0.269288  \n",
      "2w                  0.855141        0.708688  0.740045 -0.353233  0.307379  \n",
      "3w                  0.241433       -0.145658  0.183729 -0.044675 -0.009502  \n",
      "LCV                 0.276779        0.600918  0.217199 -0.074841  0.193743  \n",
      "Heavy traffic       0.023944        0.614010  0.032755  0.003525  0.177434  \n",
      "%HT                -0.645022       -0.129868 -0.532408  0.270855 -0.049840  \n",
      "Median width        1.000000        0.700068  0.840365 -0.378260  0.370328  \n",
      "Shoulder width      0.700068        1.000000  0.579699 -0.235449  0.323266  \n",
      "Temp.               0.840365        0.579699  1.000000 -0.753677  0.313976  \n",
      "RH                 -0.378260       -0.235449 -0.753677  1.000000 -0.071735  \n",
      "WS                  0.370328        0.323266  0.313976 -0.071735  1.000000  \n",
      "\n",
      "Highly correlated columns (correlation > 0.8):\n",
      "L10 and Leq: 0.87\n",
      "L50 and Leq: 0.88\n",
      "L50 and L10: 0.96\n",
      "L90 and Leq: 0.86\n",
      "L90 and L10: 0.96\n",
      "L90 and L50: 0.98\n",
      "Median width and 2w: 0.86\n",
      "Shoulder width and Leq: 0.84\n",
      "Shoulder width and L10: 0.84\n",
      "Shoulder width and L50: 0.90\n",
      "Shoulder width and L90: 0.85\n",
      "Temp. and Median width: 0.84\n"
     ]
    }
   ],
   "source": [
    "# # Convert columns with string numbers to proper numeric values\n",
    "# df = df.replace(',', '.', regex=True)  # Replace commas with dots for decimal separation\n",
    "# df = df.apply(pd.to_numeric, errors='coerce')  # Convert all columns to numeric, setting errors to NaN for non-numeric values\n",
    "\n",
    "# # Calculate the correlation matrix\n",
    "# correlation_matrix = df.corr()\n",
    "\n",
    "# # Display the correlation matrix\n",
    "# print(\"Correlation matrix:\\n\", correlation_matrix)\n",
    "\n",
    "# # Set a threshold for high correlation (e.g., 0.8)\n",
    "# threshold = 0.8\n",
    "\n",
    "# # Find pairs of columns with correlation greater than the threshold\n",
    "# high_corr_pairs = []\n",
    "\n",
    "# for i in range(len(correlation_matrix.columns)):\n",
    "#     for j in range(i):\n",
    "#         if abs(correlation_matrix.iloc[i, j]) > threshold:  # Check absolute correlation\n",
    "#             colname1 = correlation_matrix.columns[i]\n",
    "#             colname2 = correlation_matrix.columns[j]\n",
    "#             high_corr_pairs.append((colname1, colname2, correlation_matrix.iloc[i, j]))\n",
    "\n",
    "# # Print highly correlated pairs\n",
    "# if high_corr_pairs:\n",
    "#     print(\"\\nHighly correlated columns (correlation > 0.8):\")\n",
    "#     for col1, col2, corr_value in high_corr_pairs:\n",
    "#         print(f\"{col1} and {col2}: {corr_value:.2f}\")\n",
    "# else:\n",
    "#     print(\"No highly correlated columns found with correlation greater than 0.8.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop due to high correlation:\n",
      " ['L10', 'L50', 'L90', 'Median width', 'Shoulder width', 'Temp.']\n",
      "\n",
      "DataFrame after dropping highly correlated columns:\n",
      "            Leq     Speed   Car   2w  3w  LCV  Heavy traffic       %HT  RH  WS\n",
      "0    72.582436  61.808820  210  237   6   10             17  3.541667  50  11\n",
      "1    72.725820  59.440556  207  219   6   15             20  4.282655  51  12\n",
      "2    72.576617  60.354065  200  236   2   11             19  4.059829  52  13\n",
      "3    72.097628  63.814342  237  262   5   13             20  3.724395  53  14\n",
      "4    72.446424  67.122706  252  260  10    7             18  3.290676  54  15\n",
      "..         ...        ...  ...  ...  ..  ...            ...       ...  ..  ..\n",
      "455  79.191706  67.733449  532  318   7   58             68  6.917599  40  20\n",
      "456  78.006703  69.533363  446  344   4   31             85  9.340659  37  15\n",
      "457  79.325576  69.191092  548  278   1   35             79  8.395324  36  15\n",
      "458  79.978307  67.314046  522  393   3   44             64  6.237817  34  15\n",
      "459  79.404553  64.849545  527  372   4   34             70  6.951341  38  15\n",
      "\n",
      "[460 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # Import NumPy\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"C:/Users/Alok Kumar Pandey/Downloads/Book1.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Convert columns with string numbers to proper numeric values\n",
    "df = df.replace(',', '.', regex=True)  # Replace commas with dots for decimal separation\n",
    "df = df.apply(pd.to_numeric, errors='coerce')  # Convert all columns to numeric, setting errors to NaN for non-numeric values\n",
    "\n",
    "# Drop columns that are completely non-numeric or filled with NaN after conversion\n",
    "df_cleaned = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_cleaned.corr()\n",
    "\n",
    "# Set a correlation threshold\n",
    "threshold = 0.8\n",
    "\n",
    "# Create an upper triangle correlation matrix to avoid duplicate pairs\n",
    "upper_triangle = correlation_matrix.where(\n",
    "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))  # Using NumPy here\n",
    "\n",
    "# Find columns with a correlation above the threshold\n",
    "columns_to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column].abs() > threshold)]\n",
    "\n",
    "# Drop highly correlated columns\n",
    "df_reduced = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "# Print the columns to drop and the resulting DataFrame\n",
    "print(\"Columns to drop due to high correlation:\\n\", columns_to_drop)\n",
    "print(\"\\nDataFrame after dropping highly correlated columns:\\n\", df_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alok Kumar Pandey\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: {'loss': 5698.52685546875, 'val_loss': 5642.6240234375}\n",
      "Epoch 2: {'loss': 5598.47265625, 'val_loss': 5542.107421875}\n",
      "Epoch 3: {'loss': 5489.91162109375, 'val_loss': 5424.58349609375}\n",
      "Epoch 4: {'loss': 5356.46826171875, 'val_loss': 5272.0361328125}\n",
      "Epoch 5: {'loss': 5180.822265625, 'val_loss': 5068.275390625}\n",
      "Epoch 6: {'loss': 4950.66455078125, 'val_loss': 4805.423828125}\n",
      "Epoch 7: {'loss': 4656.384765625, 'val_loss': 4469.248046875}\n",
      "Epoch 8: {'loss': 4285.85888671875, 'val_loss': 4057.744140625}\n",
      "Epoch 9: {'loss': 3842.3095703125, 'val_loss': 3570.68994140625}\n",
      "Epoch 10: {'loss': 3329.353759765625, 'val_loss': 3019.860107421875}\n",
      "Epoch 11: {'loss': 2760.737548828125, 'val_loss': 2428.920654296875}\n",
      "Epoch 12: {'loss': 2170.19580078125, 'val_loss': 1836.3973388671875}\n",
      "Epoch 13: {'loss': 1599.7210693359375, 'val_loss': 1290.84130859375}\n",
      "Epoch 14: {'loss': 1097.771728515625, 'val_loss': 846.4442138671875}\n",
      "Epoch 15: {'loss': 716.6375122070312, 'val_loss': 532.8558959960938}\n",
      "Epoch 16: {'loss': 461.2701416015625, 'val_loss': 348.2476806640625}\n",
      "Epoch 17: {'loss': 322.28875732421875, 'val_loss': 262.4267272949219}\n",
      "Epoch 18: {'loss': 261.0688171386719, 'val_loss': 232.17591857910156}\n",
      "Epoch 19: {'loss': 238.27638244628906, 'val_loss': 222.64109802246094}\n",
      "Epoch 20: {'loss': 225.35572814941406, 'val_loss': 213.91311645507812}\n",
      "Epoch 21: {'loss': 216.16177368164062, 'val_loss': 207.14340209960938}\n",
      "Epoch 22: {'loss': 205.32562255859375, 'val_loss': 200.3182373046875}\n",
      "Epoch 23: {'loss': 196.29779052734375, 'val_loss': 195.836669921875}\n",
      "Epoch 24: {'loss': 188.07867431640625, 'val_loss': 190.03927612304688}\n",
      "Epoch 25: {'loss': 179.77574157714844, 'val_loss': 183.2908477783203}\n",
      "Epoch 26: {'loss': 172.63462829589844, 'val_loss': 175.072265625}\n",
      "Epoch 27: {'loss': 165.66563415527344, 'val_loss': 170.5979766845703}\n",
      "Epoch 28: {'loss': 159.56382751464844, 'val_loss': 165.80604553222656}\n",
      "Epoch 29: {'loss': 153.66424560546875, 'val_loss': 160.7773895263672}\n",
      "Epoch 30: {'loss': 147.74290466308594, 'val_loss': 155.593017578125}\n",
      "Epoch 31: {'loss': 142.65640258789062, 'val_loss': 152.22543334960938}\n",
      "Epoch 32: {'loss': 137.3009033203125, 'val_loss': 148.1153106689453}\n",
      "Epoch 33: {'loss': 133.0825958251953, 'val_loss': 142.85633850097656}\n",
      "Epoch 34: {'loss': 127.8457260131836, 'val_loss': 137.77178955078125}\n",
      "Epoch 35: {'loss': 123.73095703125, 'val_loss': 134.1435089111328}\n",
      "Epoch 36: {'loss': 119.53057098388672, 'val_loss': 131.04664611816406}\n",
      "Epoch 37: {'loss': 115.55156707763672, 'val_loss': 127.81763458251953}\n",
      "Epoch 38: {'loss': 111.86946105957031, 'val_loss': 125.26963806152344}\n",
      "Epoch 39: {'loss': 108.13091278076172, 'val_loss': 122.53231811523438}\n",
      "Epoch 40: {'loss': 104.69369506835938, 'val_loss': 118.13711547851562}\n",
      "Epoch 41: {'loss': 100.97149658203125, 'val_loss': 115.57622528076172}\n",
      "Epoch 42: {'loss': 98.7259750366211, 'val_loss': 114.62711334228516}\n",
      "Epoch 43: {'loss': 95.81975555419922, 'val_loss': 112.24810028076172}\n",
      "Epoch 44: {'loss': 93.03015899658203, 'val_loss': 109.47251892089844}\n",
      "Epoch 45: {'loss': 90.51406860351562, 'val_loss': 107.72162628173828}\n",
      "Epoch 46: {'loss': 87.83788299560547, 'val_loss': 104.95781707763672}\n",
      "Epoch 47: {'loss': 85.51754760742188, 'val_loss': 101.5677261352539}\n",
      "Epoch 48: {'loss': 83.21245574951172, 'val_loss': 98.43383026123047}\n",
      "Epoch 49: {'loss': 81.23595428466797, 'val_loss': 96.5476303100586}\n",
      "Epoch 50: {'loss': 78.96129608154297, 'val_loss': 96.08268737792969}\n",
      "Epoch 51: {'loss': 76.97314453125, 'val_loss': 94.87730407714844}\n",
      "Epoch 52: {'loss': 74.97565460205078, 'val_loss': 93.1441650390625}\n",
      "Epoch 53: {'loss': 73.20405578613281, 'val_loss': 90.321533203125}\n",
      "Epoch 54: {'loss': 71.4755630493164, 'val_loss': 87.94788360595703}\n",
      "Epoch 55: {'loss': 69.63245391845703, 'val_loss': 86.7076644897461}\n",
      "Epoch 56: {'loss': 67.8102798461914, 'val_loss': 84.84073638916016}\n",
      "Epoch 57: {'loss': 66.40250396728516, 'val_loss': 83.42777252197266}\n",
      "Epoch 58: {'loss': 64.72369384765625, 'val_loss': 81.3810806274414}\n",
      "Epoch 59: {'loss': 63.1741943359375, 'val_loss': 79.82721710205078}\n",
      "Epoch 60: {'loss': 61.832611083984375, 'val_loss': 78.67279052734375}\n",
      "Epoch 61: {'loss': 60.53834533691406, 'val_loss': 77.43284606933594}\n",
      "Epoch 62: {'loss': 59.1413688659668, 'val_loss': 75.9270248413086}\n",
      "Epoch 63: {'loss': 58.0191650390625, 'val_loss': 74.54185485839844}\n",
      "Epoch 64: {'loss': 56.66869354248047, 'val_loss': 73.8876724243164}\n",
      "Epoch 65: {'loss': 55.56572723388672, 'val_loss': 73.28385925292969}\n",
      "Epoch 66: {'loss': 54.21708297729492, 'val_loss': 72.21250915527344}\n",
      "Epoch 67: {'loss': 53.05177307128906, 'val_loss': 70.52316284179688}\n",
      "Epoch 68: {'loss': 52.01296615600586, 'val_loss': 69.11286163330078}\n",
      "Epoch 69: {'loss': 50.832977294921875, 'val_loss': 68.3682861328125}\n",
      "Epoch 70: {'loss': 49.86184310913086, 'val_loss': 67.05694580078125}\n",
      "Epoch 71: {'loss': 48.9124755859375, 'val_loss': 66.06307983398438}\n",
      "Epoch 72: {'loss': 47.712032318115234, 'val_loss': 63.56536865234375}\n",
      "Epoch 73: {'loss': 46.84340286254883, 'val_loss': 63.120487213134766}\n",
      "Epoch 74: {'loss': 45.56947708129883, 'val_loss': 62.121681213378906}\n",
      "Epoch 75: {'loss': 44.735626220703125, 'val_loss': 60.94580078125}\n",
      "Epoch 76: {'loss': 43.758056640625, 'val_loss': 60.37698745727539}\n",
      "Epoch 77: {'loss': 42.91461944580078, 'val_loss': 59.49553298950195}\n",
      "Epoch 78: {'loss': 41.991455078125, 'val_loss': 58.271522521972656}\n",
      "Epoch 79: {'loss': 41.196739196777344, 'val_loss': 57.5696907043457}\n",
      "Epoch 80: {'loss': 40.16328430175781, 'val_loss': 57.315086364746094}\n",
      "Epoch 81: {'loss': 39.58797836303711, 'val_loss': 56.14800262451172}\n",
      "Epoch 82: {'loss': 38.61997604370117, 'val_loss': 54.512760162353516}\n",
      "Epoch 83: {'loss': 37.65971374511719, 'val_loss': 52.83907699584961}\n",
      "Epoch 84: {'loss': 37.19982147216797, 'val_loss': 52.543277740478516}\n",
      "Epoch 85: {'loss': 36.259857177734375, 'val_loss': 51.80181121826172}\n",
      "Epoch 86: {'loss': 35.491634368896484, 'val_loss': 51.4677848815918}\n",
      "Epoch 87: {'loss': 34.322940826416016, 'val_loss': 50.299251556396484}\n",
      "Epoch 88: {'loss': 33.53084945678711, 'val_loss': 48.49603271484375}\n",
      "Epoch 89: {'loss': 32.770267486572266, 'val_loss': 47.90697479248047}\n",
      "Epoch 90: {'loss': 32.242767333984375, 'val_loss': 47.63187026977539}\n",
      "Epoch 91: {'loss': 31.56770896911621, 'val_loss': 46.22589874267578}\n",
      "Epoch 92: {'loss': 31.324155807495117, 'val_loss': 46.02798843383789}\n",
      "Epoch 93: {'loss': 30.428401947021484, 'val_loss': 44.508235931396484}\n",
      "Epoch 94: {'loss': 29.64847755432129, 'val_loss': 44.03480911254883}\n",
      "Epoch 95: {'loss': 29.24956512451172, 'val_loss': 42.99130630493164}\n",
      "Epoch 96: {'loss': 28.620685577392578, 'val_loss': 42.377376556396484}\n",
      "Epoch 97: {'loss': 28.01166534423828, 'val_loss': 42.37336730957031}\n",
      "Epoch 98: {'loss': 27.520954132080078, 'val_loss': 41.40261459350586}\n",
      "Epoch 99: {'loss': 26.905681610107422, 'val_loss': 41.054481506347656}\n",
      "Epoch 100: {'loss': 26.457101821899414, 'val_loss': 40.059268951416016}\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "ANN Results:\n",
      "MSE: 67.04953282760538\n",
      "MAE: 5.2734545256106555\n",
      "RMSE: 8.188377911870298\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109us/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n",
      "Feature Importances (Permutation Importance):\n",
      "Median width      138.841816\n",
      "Shoulder width    102.952432\n",
      "Temp.              48.336329\n",
      "%HT                37.955737\n",
      "Heavy traffic      26.454621\n",
      "RH                 20.375057\n",
      "2w                 17.940158\n",
      "L90                11.140945\n",
      "WS                  6.608988\n",
      "Car                 6.259398\n",
      "Speed               3.724028\n",
      "LCV                 0.681435\n",
      "3w                 -0.264957\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Load the data (replace with your file path)\n",
    "file_path = 'reduced_data_traffic.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Defining the features (all columns except Leq) and target (Leq)\n",
    "X = data.drop(columns=['Leq'])\n",
    "y = data['Leq']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple ANN model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[X_train_scaled.shape[1]]),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with an optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Create a list to store the training log\n",
    "training_logs = []\n",
    "\n",
    "# Custom callback to log the training output\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        training_logs.append(logs)\n",
    "\n",
    "# Train the model with the custom callback\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, validation_split=0.2, verbose=0, callbacks=[CustomCallback()])\n",
    "\n",
    "# Display the complete training logs\n",
    "for epoch, log in enumerate(training_logs):\n",
    "    print(f\"Epoch {epoch + 1}: {log}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the MSE, MAE, and RMSE\n",
    "mse_ann = mean_squared_error(y_test, y_pred)\n",
    "mae_ann = mean_absolute_error(y_test, y_pred)\n",
    "rmse_ann = np.sqrt(mse_ann)\n",
    "\n",
    "print(f\"ANN Results:\\nMSE: {mse_ann}\\nMAE: {mae_ann}\\nRMSE: {rmse_ann}\")\n",
    "\n",
    "# Create a wrapper class for the Keras model to use with sklearn's permutation_importance\n",
    "class KerasRegressorWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y, epochs=100, verbose=0):\n",
    "        self.model.fit(X, y, epochs=epochs, verbose=verbose)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X).flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return -mean_squared_error(y, self.predict(X))  # Returning negative MSE as score\n",
    "\n",
    "# Wrap the Keras model\n",
    "wrapped_model = KerasRegressorWrapper(model)\n",
    "\n",
    "# Fit the wrapped model\n",
    "wrapped_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Apply permutation importance on the trained ANN model\n",
    "perm_importance = permutation_importance(wrapped_model, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Displaying feature importances\n",
    "feature_importances = pd.Series(perm_importance.importances_mean, index=X.columns).sort_values(ascending=False)\n",
    "print(\"Feature Importances (Permutation Importance):\")\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
